{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1de2bdd-7062-4afa-b5bc-80e6c274cb16",
   "metadata": {},
   "source": [
    "# What New Things Learn Me.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08fa85e-7286-434d-9c2b-1bffb305d448",
   "metadata": {},
   "source": [
    "This dataset is basically:\n",
    "\n",
    "* 🏗️ **Bulldozers (and other heavy equipment)** being **sold at auctions** across the **United States**.\n",
    "* Each row = **one machine auction event**.\n",
    "* Columns = machine characteristics (model, year, size, hours used, features, etc.) + auction info (date, state, auctioneer).\n",
    "* Target = **final sale price at that auction**.\n",
    "\n",
    "So your task is to **predict the auction price of a bulldozer before it’s sold**, based on:\n",
    "\n",
    "* What it is (machine type, size, model year).\n",
    "* Where it’s sold (state, auctioneer).\n",
    "* When it’s sold (auction date).\n",
    "* How much it’s been used (hours, usage band).\n",
    "\n",
    "👉 In real life, this is valuable for:\n",
    "\n",
    "* **Auction companies** → set reserve prices.\n",
    "* **Buyers** → estimate fair value.\n",
    "* **Dealers** → predict market trends.\n",
    "\n",
    "---\n",
    "---\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290e49e-8c8b-4f11-9020-21046e3f9883",
   "metadata": {},
   "source": [
    " **differences in how datasets are split**. Let’s compare with your **Heart Disease project** vs. the **Bulldozer Price project**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🫀 Heart Disease Dataset\n",
    "\n",
    "* It’s a **small dataset** (303 rows).\n",
    "* Usually we just split into **train + test** using `train_test_split`.\n",
    "* The test set acts like your validation → because the dataset is small, there’s no official “validation” file given.\n",
    "\n",
    "So:\n",
    "\n",
    "* Train = used for fitting the model\n",
    "* Test = used for final evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## 🚜 Bulldozer Price Dataset (Kaggle)\n",
    "\n",
    "* It’s a **large dataset** (hundreds of thousands of rows).\n",
    "* Kaggle competition provides **train.csv, valid.csv, test.csv** separately.\n",
    "* Reason: this dataset is **time-series like** → auctions happen at different dates.\n",
    "\n",
    "  * Training = older data\n",
    "  * Validation = newer unseen auctions (closer to test period)\n",
    "  * Test = the “future” (your final predictions)\n",
    "\n",
    "👉 So, we need **validation** to simulate how well the model predicts “future” prices.\n",
    "If we used random split like in Heart Disease, it would mix up old and future auctions, which is unrealistic.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Difference in Purpose\n",
    "\n",
    "* **Validation set** → Helps you tune your model and check performance **before seeing the test set**.\n",
    "* **Test set** → Only used at the very end for the final score (like Kaggle leaderboard).\n",
    "\n",
    "📌 Think of it like:\n",
    "\n",
    "* Train = learning notes for an exam\n",
    "* Validation = practice exam\n",
    "* Test = the real exam\n",
    "\n",
    "In **Heart Disease**, practice exam (validation) wasn’t strictly necessary because dataset was small and not time-dependent.\n",
    "In **Bulldozers**, it’s **required** because it’s time-series style and Kaggle structured it that way.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec0819-4542-4965-8e0c-c53291e764c6",
   "metadata": {},
   "source": [
    "https://chatgpt.com/backend-api/estuary/content?id=file-BgWVWz2Ros3RpFc5pieADK&ts=487837&p=fs&cid=1&sig=ab0bd6388c0be0fc81aa1ec7b18a3df182435924807560d805da5950bceb2d59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b232b70-9e4c-459b-9eb9-8a99ec549ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "<img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1c4e8-f3b5-4e80-bc5b-f2c58377412a",
   "metadata": {},
   "source": [
    "> missing value summary for the Bulldozer dataset\n",
    "---\n",
    "\n",
    "## 🗂️ Columns Explanation\n",
    "\n",
    "### ✅ Always Present (no missing values)\n",
    "\n",
    "* **SalesID** → unique identifier for each sale (not useful for prediction).\n",
    "* **SalePrice** → target variable (what we predict).\n",
    "* **MachineID** → ID for each machine (can help track repeated machines).\n",
    "* **ModelID** → ID for machine’s model.\n",
    "* **datasource** → origin of data (less important).\n",
    "* **YearMade** → year machine was manufactured.\n",
    "* **saledate** → auction date (very important for time split).\n",
    "* **fiModelDesc**, **fiBaseModel**, **fiProductClassDesc**, **state**, **ProductGroup**, **ProductGroupDesc** → all categorical descriptors about machine type.\n",
    "\n",
    "These are safe features, always available.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Partially Missing\n",
    "\n",
    "* **auctioneerID (20k missing)** → who conducted the auction. Might matter (different auctioneers, different prices). Could fill missing as `\"Unknown\"`.\n",
    "* **MachineHoursCurrentMeter (265k missing)** → how many hours the machine has worked. Super useful (like car mileage). Missing a lot though. Might need special handling.\n",
    "* **UsageBand (339k missing)** → categorical (High/Medium/Low usage). Basically derived from hours → so missing a lot.\n",
    "* **fiSecondaryDesc, fiModelSeries, fiModelDescriptor (140k–350k missing)** → extra model description details.\n",
    "* **ProductSize (216k missing)** → size of machine (Small, Medium, Large). Important, but lots missing.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚨 Very High Missing (almost useless unless carefully handled)\n",
    "\n",
    "These have **300k+ missing values** (dataset total is \\~400k rows):\n",
    "\n",
    "* **Drive\\_System, Forks, Pad\\_Type, Ride\\_Control, Stick, Transmission, Turbocharged**\n",
    "* **Blade\\_Extension, Blade\\_Width, Enclosure\\_Type, Engine\\_Horsepower, Pushblock, Ripper, Scarifier, Tip\\_Control, Tire\\_Size, Coupler, Coupler\\_System, Grouser\\_Tracks, Hydraulics\\_Flow, Track\\_Type, Undercarriage\\_Pad\\_Width, Stick\\_Length, Thumb, Pattern\\_Changer, Grouser\\_Type, Backhoe\\_Mounting, Blade\\_Type, Travel\\_Controls, Differential\\_Type, Steering\\_Controls**\n",
    "\n",
    "👉 Many of these are *equipment options/features* (like whether machine has forks, turbocharged engine, blade type, etc.). They can be useful, but because they’re missing so much, including them may hurt model performance unless we impute `\"Unknown\"` or drop them.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 What to Do with These Columns\n",
    "\n",
    "1. **Keep essential ones**:\n",
    "\n",
    "   * `saledate`, `YearMade`, `MachineHoursCurrentMeter`, `ProductSize`, `fiBaseModel`, `state`, `ProductGroup`.\n",
    "2. **For missing categorical columns**: fill with `\"Unknown\"`.\n",
    "3. **For missing numerical columns**: fill with `median` or `0` (and add a flag column like `HasHours = 1/0`).\n",
    "4. **Drop very high-missing columns (optional)**: If >80% missing (like Blade\\_Extension), test both dropping and keeping them.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Key Insight\n",
    "\n",
    "* In Kaggle solutions, most **feature engineering comes from `saledate` + `YearMade` + `MachineHoursCurrentMeter`**.\n",
    "* Features like `auctioneerID` and `ProductSize` also add value.\n",
    "* Many of those 300k-missing option columns are **too sparse** to help much, but you can try.\n",
    "\n",
    "---\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca18631-4f40-44dd-be8c-dca0de006394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59ec4056-832c-47ee-8138-e3a46eecdd16",
   "metadata": {},
   "source": [
    "> low_memory=False, parse_dates=[\"saledate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1213be-8b04-41b2-bf38-6adbce4ce959",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "df = pd.read_csv(\"data/TrainAndValid.csv\", low_memory=False, parse_dates=[\"saledate\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 Step by Step\n",
    "\n",
    "### 1. `pd.read_csv(\"data/TrainAndValid.csv\")`\n",
    "\n",
    "* Reads your CSV file into a **Pandas DataFrame** called `df`.\n",
    "* Each column becomes a Series (like a labeled array).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `low_memory=False`\n",
    "\n",
    "* By default, Pandas tries to **guess the data types in chunks** to save memory.\n",
    "* With big files, this sometimes causes mixed dtypes (like one column partly `int`, partly `object`).\n",
    "* Setting `low_memory=False` tells Pandas:\n",
    "\n",
    "  > “Read the whole file first, then figure out the best data types.”\n",
    "* ✅ More accurate column types, but slightly slower load time.\n",
    "* Useful for large datasets like this Bulldozer dataset (\\~400k rows, many columns).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `parse_dates=[\"saledate\"]`\n",
    "\n",
    "* Normally, CSV stores dates as **strings** (e.g., `\"2009-11-17 00:00:00\"`).\n",
    "* This option tells Pandas:\n",
    "\n",
    "  > “When loading, automatically convert the `saledate` column into `datetime64[ns]` type.”\n",
    "* ✅ This makes it much easier to do **time-based analysis** later:\n",
    "\n",
    "  * Extract year, month, day → `df[\"saledate\"].dt.year`\n",
    "  * Filter by date → `df[df[\"saledate\"] > \"2010-01-01\"]`\n",
    "  * Plot trends over time\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ In Short\n",
    "\n",
    "That line gives you:\n",
    "\n",
    "* `df` = clean DataFrame\n",
    "* `saledate` already in datetime format (ready for feature engineering)\n",
    "* fewer dtype guessing issues because of `low_memory=False`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c91a250-fc1e-4a19-a3ef-a1f44b3ff1ad",
   "metadata": {},
   "source": [
    ">\n",
    ">---\n",
    ">\n",
    "### Sort By Date\n",
    "df.sort_values(by=[\"saledate\"] , inplace = True , ascending = True)\n",
    "\n",
    "### Make a copy\n",
    "df_tmp = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852089ba-7c62-4df5-9897-cc09f375042e",
   "metadata": {},
   "source": [
    "> Deeply Meaning of each Column "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49ba55-70d5-4199-9f99-ab68dc433499",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🔑 Meaning of Important Columns\n",
    "\n",
    "### Identification Columns\n",
    "\n",
    "* **SalesID** → Unique ID for each auction (not useful for prediction directly).\n",
    "* **MachineID** → Unique ID for the machine (same machine may appear in multiple auctions).\n",
    "* **ModelID** → ID of the machine model (groups machines into categories).\n",
    "* **datasource** → Where the data came from (not usually meaningful).\n",
    "\n",
    "---\n",
    "\n",
    "### Target\n",
    "\n",
    "* **SalePrice** → Bulldozer’s sale price at auction (what we predict).\n",
    "\n",
    "---\n",
    "\n",
    "### Auction Information\n",
    "\n",
    "* **saledate** → Date of auction (very important — it’s a time series).\n",
    "* **auctioneerID** → ID of the auctioneer company that ran the auction (some companies may attract higher bids).\n",
    "* **state** → U.S. state where the auction took place (location effect).\n",
    "  👉 This does **not mean who bought it**, but **where it was sold**.\n",
    "  For example:\n",
    "\n",
    "  * TX → Texas\n",
    "  * CA → California\n",
    "  * FL → Florida\n",
    "    Prices might vary by state (due to demand).\n",
    "\n",
    "---\n",
    "\n",
    "### Machine Info\n",
    "\n",
    "* **YearMade** → Year the machine was manufactured.\n",
    "* **MachineHoursCurrentMeter** → Number of hours the machine has been used (like mileage for cars).\n",
    "* **UsageBand** → Categorized usage level (High/Medium/Low) — often derived from hours.\n",
    "* **ProductSize** → Size class of machine (Small, Medium, Large, etc.).\n",
    "* **ProductGroup / ProductGroupDesc** → Broad category of equipment (e.g., Wheel Loader, Track Type Tractor).\n",
    "\n",
    "---\n",
    "\n",
    "### Model Descriptors\n",
    "\n",
    "* **fiModelDesc** → Full description of the machine model.\n",
    "* **fiBaseModel** → Base model (more general).\n",
    "* **fiSecondaryDesc / fiModelSeries / fiModelDescriptor** → Extra details about the model type.\n",
    "* **fiProductClassDesc** → Product class description (category, e.g., \"Backhoe Loader - 4WD\").\n",
    "\n",
    "---\n",
    "\n",
    "### Equipment Features (lots of missing data!)\n",
    "\n",
    "* **Drive\\_System** → Type of drive (2WD, 4WD, etc.).\n",
    "* **Enclosure** → Type of cab (open, enclosed, etc.).\n",
    "* **Forks** → Whether machine has forks.\n",
    "* **Pad\\_Type** → Track pad type.\n",
    "* **Ride\\_Control** → Has ride control system?\n",
    "* **Stick** → Type of stick (for excavators).\n",
    "* **Transmission** → Transmission type.\n",
    "* **Turbocharged** → Engine turbocharged or not.\n",
    "* **Blade\\_Extension / Blade\\_Width / Blade\\_Type** → Bulldozer blade details.\n",
    "* **Engine\\_Horsepower** → Engine power rating.\n",
    "* **Hydraulics / Hydraulics\\_Flow** → Type of hydraulic system.\n",
    "* **Ripper, Scarifier, Coupler, Thumb, Pattern\\_Changer, Pushblock** → Additional attachments.\n",
    "* **Track\\_Type / Undercarriage\\_Pad\\_Width / Grouser\\_Type / Grouser\\_Tracks** → Track and undercarriage details.\n",
    "* **Travel\\_Controls, Differential\\_Type, Steering\\_Controls** → Control systems.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Key Understanding\n",
    "\n",
    "* `state` = **location of auction** (not buyer).\n",
    "* Many “equipment option” columns are **optional features** (like car add-ons: sunroof, turbo engine).\n",
    "* For modeling, we usually focus on:\n",
    "\n",
    "  * `saledate` (turn into Year/Month/Day)\n",
    "  * `YearMade`\n",
    "  * `MachineHoursCurrentMeter`\n",
    "  * `ProductSize`\n",
    "  * `ProductGroup`\n",
    "  * `state`\n",
    "  * `auctioneerID`\n",
    "\n",
    "The rest can help, but they’re very sparse (lots of missing).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4ed6c-b0ea-41e3-9aa9-3e99eaae78df",
   "metadata": {},
   "source": [
    "---\n",
    ">\n",
    "\n",
    "🔥 \n",
    "---\n",
    "\n",
    "### Your Code\n",
    "\n",
    "```python\n",
    "for label, content in df_tmp.items():\n",
    "    if pd.api.types.is_object_dtype(content):\n",
    "        df_tmp[label] = content.astype(\"category\").cat.as_ordered()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 What it does\n",
    "\n",
    "1. **Check each column** → if it’s `object` dtype (strings).\n",
    "2. Convert it to **category dtype**.\n",
    "3. Mark the category as **ordered** (so Pandas knows the categories have a meaningful order).\n",
    "\n",
    "---\n",
    "\n",
    "### ❓ Why convert string → category?\n",
    "\n",
    "1. **Memory efficiency**\n",
    "\n",
    "   * Strings take lots of space (each cell stores full text).\n",
    "   * Category stores them as **integer codes + lookup table**.\n",
    "   * Example:\n",
    "\n",
    "     ```\n",
    "     \"High\", \"Medium\", \"Low\", \"High\", \"Low\"\n",
    "     ```\n",
    "\n",
    "     → stored internally as `[2, 1, 0, 2, 0]` + mapping `{0:\"Low\",1:\"Medium\",2:\"High\"}`.\n",
    "   * Huge memory savings in big datasets like Bulldozers (\\~400k rows).\n",
    "\n",
    "2. **Speed**\n",
    "\n",
    "   * Operations like `.value_counts()`, groupby, comparisons are **much faster** on categories.\n",
    "\n",
    "3. **Prepares for ML**\n",
    "\n",
    "   * Most ML models need **numbers**, not strings.\n",
    "   * Once it’s category, you can easily convert to codes:\n",
    "\n",
    "     ```python\n",
    "     df_tmp[label] = df_tmp[label].cat.codes + 1\n",
    "     ```\n",
    "\n",
    "     (add `+1` so missing stays as `-1` → now `0` = unknown).\n",
    "\n",
    "---\n",
    "\n",
    "### ❓ Why `.cat.as_ordered()`?\n",
    "\n",
    "* By default, categories are **unordered** (just labels).\n",
    "* `as_ordered()` tells Pandas: “treat these categories as ordered” (like Low < Medium < High).\n",
    "* This matters if the categories have a natural order (like `UsageBand`).\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "df_tmp[\"UsageBand\"].cat.categories\n",
    "# Index(['High', 'Low', 'Medium'], dtype='object')\n",
    "\n",
    "df_tmp[\"UsageBand\"].cat.as_ordered()\n",
    "```\n",
    "\n",
    "Now you can sort, compare (`Low < Medium < High`), etc.\n",
    "\n",
    "👉 But ⚠️ not all categories are *truly ordered*.\n",
    "\n",
    "* `state` (CA, TX, NY) → unordered (just labels).\n",
    "* `UsageBand` (Low, Medium, High) → ordered.\n",
    "\n",
    "So in practice, you only want `as_ordered()` on meaningful cases.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "We convert string `object` → `category` because:\n",
    "\n",
    "* Saves memory.\n",
    "* Makes operations faster.\n",
    "* Makes it easy to encode into numbers for ML.\n",
    "* `as_ordered()` is extra helpful when categories have a natural rank (like Low < Med < High).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ce605-a460-4eef-8d90-3affcdf58168",
   "metadata": {},
   "source": [
    "> Mean And median Difference \n",
    ">\n",
    "---\n",
    "\n",
    "### 📊 Mean (Average)\n",
    "\n",
    "The **mean** is the \"average\" of all numbers.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Mean} = \\frac{\\text{Sum of all values}}{\\text{Number of values}}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Values = [2, 3, 4, 10, 50]\n",
    "\n",
    "Mean = (2 + 3 + 4 + 10 + 50) / 5 = 69 / 5 = 13.8\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Median\n",
    "\n",
    "The **median** is the \"middle\" value when numbers are sorted.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Sort values\n",
    "2. Pick the middle one\n",
    "\n",
    "   * If odd count → exact middle\n",
    "   * If even count → average of two middle numbers\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Values = [2, 3, 4, 10, 50]\n",
    "Sorted → [2, 3, 4, 10, 50]\n",
    "Median = 4  (middle value)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔑 Key Difference\n",
    "\n",
    "* **Mean** is affected by **outliers** (very high/low values).\n",
    "* **Median** is more **robust** to outliers.\n",
    "\n",
    "Example with outlier:\n",
    "\n",
    "```\n",
    "Values = [2, 3, 4, 10, 5000]\n",
    "\n",
    "Mean = (2+3+4+10+5000)/5 = 1003.8\n",
    "Median = 4\n",
    "```\n",
    "\n",
    "⚠️ Here, mean ≈ 1004, but most values are small. Median (4) gives a more realistic \"center\".\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ In ML Preprocessing\n",
    "\n",
    "* If data is **normally distributed** (no big outliers) → Mean is fine.\n",
    "* If data has **outliers or skewed distribution** → Median is better.\n",
    "\n",
    "That’s why in your bulldozer dataset, we usually fill missing numeric values with **median**, not mean.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e805d-a13c-4239-a41e-bf39ad4f2cda",
   "metadata": {},
   "source": [
    "---\n",
    "> %%time\n",
    "\n",
    "\n",
    "\n",
    "In Jupyter Notebooks (like the one I am using for your course),\n",
    "\n",
    "`%%time` is a **cell magic command** that measures how long the whole cell takes to run.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "%%time\n",
    "# Some code\n",
    "for i in range(10**6):\n",
    "    _ = i**2\n",
    "```\n",
    "\n",
    "Output might be:\n",
    "\n",
    "```\n",
    "CPU times: user 200 ms, sys: 10 ms, total: 210 ms\n",
    "Wall time: 210 ms\n",
    "```\n",
    "\n",
    "* **CPU times** → How much time the CPU spent running your code.\n",
    "* **Wall time** → The actual time you waited (real-world time).\n",
    "\n",
    "There’s also `%time` (single `%`), which is for **one line only**:\n",
    "\n",
    "```python\n",
    "%time sum([i**2 for i in range(10**6)])\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79e0c4-7e3d-4a2d-b9c8-127291eeb222",
   "metadata": {},
   "source": [
    ">max samples\n",
    "\n",
    "---\n",
    "\n",
    "### 🌲 `max_samples` in `RandomForestRegressor`\n",
    "\n",
    "By default, each tree in a RandomForest is trained on a **bootstrap sample** of the full training dataset.\n",
    "\n",
    "* **Without `max_samples`** → each tree sees all the data (with replacement).\n",
    "* **With `max_samples`** → each tree only sees a **subset** of the training data.\n",
    "\n",
    "So:\n",
    "\n",
    "```python\n",
    "model = RandomForestRegressor(\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    max_samples=10000   # each tree only sees 10k rows\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Why use `max_samples`?\n",
    "\n",
    "1. **Speed up training** ⏱️\n",
    "\n",
    "   * If you have **hundreds of thousands of rows** (like the bulldozer dataset), fitting each tree on the full dataset is very slow.\n",
    "   * Limiting to 10k samples makes each tree much faster.\n",
    "\n",
    "2. **Regularization (less overfitting)** 🎯\n",
    "\n",
    "   * With fewer samples per tree, each tree becomes a bit weaker.\n",
    "   * But when you combine many of these weaker trees, the **ensemble generalizes better**.\n",
    "   * This is similar to how Dropout works in neural nets.\n",
    "\n",
    "3. **Memory efficiency** 🧠\n",
    "\n",
    "   * Training big RandomForests on the full dataset can eat up a lot of memory.\n",
    "   * Using `max_samples` reduces memory usage.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚖️ Trade-off\n",
    "\n",
    "* If `max_samples` is **too small**, each tree might not learn enough → lower accuracy.\n",
    "* If `max_samples` is **too large** (or None), training is slow and may overfit.\n",
    "\n",
    "The sweet spot is usually **10k–30k samples per tree** for large datasets like Bulldozers.\n",
    "\n",
    "---\n",
    "\n",
    "👉 So in the Bulldozer project, `max_samples=10000` is used because the dataset is **huge (400k+ rows)**, and this makes model training feasible while still giving good results.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729a44f-2b24-4bf8-bfe2-dd615fc0e2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cace6c3-9f88-4db7-8027-578093f4785c",
   "metadata": {},
   "source": [
    "### GOING TO FIND THE COLLUMN DIFFER\n",
    "\n",
    "set(x_train.columns) - set(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d30e59-31a5-471d-8408-013019ef4d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
